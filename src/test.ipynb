{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from groq import Groq\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from tqdm import trange\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import logging\n",
    "from math import ceil\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "## load evv variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "CHAT_MODEL   = os.environ[\"CHAT_MODEL\"]\n",
    "client       = Groq()\n",
    "\n",
    "GENERATE_EN_LABELS_PROMPT = '''\n",
    "You are an linguistics professor tasked with classifying seller feedback for an e-commerce platform. \n",
    "Each feedback item should be categorised into one or more appropriate labels from the following list:\n",
    "['Negative Complaint','Constructive Criticism','Design Feedback','Positive Comment','Neutral']\n",
    "You are not to write any code, but just use your knowledge to classify the feedback.\n",
    "Your output should be the feedback IDs and their corresponding label.\n",
    "\n",
    "Now classify the following feedback:\n",
    "Feedbacks: {pairs}\n",
    "\n",
    "Example Output format:\n",
    "[{{\"feedback_id\": 123456, \"label\": [\"Negative Complaint\"]}}, {{\"feedback_id\": 423456, \"label\": [\"Constructive Criticism\",\"Design Feedback\"]}}, {{\"feedback_id\": 654321, \"label\": [\"Negative Complaint\"]}}]\n",
    "\n",
    "Double check and ensure that your format output matches the example output format provided.\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_region_data(region: str) -> pd.DataFrame:\n",
    "    # Define the file path based on the region\n",
    "    region_path = f\"../data/official_data/feedback_{region}.xls\"\n",
    "\n",
    "    # Specify columns to read\n",
    "    columns_to_read = [\"Feedback id\", \"Feedback 1\", \"Feedback 2\"]\n",
    "\n",
    "    # Load the data into a DataFrame\n",
    "    df = pd.read_excel(region_path, usecols=columns_to_read)\n",
    "\n",
    "    # Filter out rows with missing or invalid data\n",
    "    df_filtered = df[\n",
    "        (df['Feedback 1'].notna()) &\n",
    "        (df['Feedback 2'].notna()) &\n",
    "        (df['Feedback 2'] != '{\"description\":\"\"}')\n",
    "    ]\n",
    "\n",
    "    # Extract the 'description' field from JSON in 'Feedback 2'\n",
    "    df_filtered['Feedback 2'] = df_filtered['Feedback 2'].apply(\n",
    "        lambda x: json.loads(x)['description'] if isinstance(x, str) else None\n",
    "    )\n",
    "\n",
    "    # Convert 'Feedback id' to numeric and drop rows with invalid IDs\n",
    "    df_filtered['Feedback id'] = pd.to_numeric(df_filtered['Feedback id'], errors='coerce')\n",
    "    df_filtered = df_filtered.dropna(subset=['Feedback id'])\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def format_llm_input(df: pd.DataFrame) -> Tuple[List[Dict[str, str]], Dict[int, str]]:\n",
    "    # Extract feedback IDs and feedback text\n",
    "    feedback_ids = list(df['Feedback id'])\n",
    "    feedback_texts = list(df['Feedback 2'])\n",
    "\n",
    "    # Create a dictionary mapping feedback IDs to feedback text\n",
    "    id_feedback = {int(feedback_id): feedback for feedback_id, feedback in zip(feedback_ids, feedback_texts)}\n",
    "\n",
    "    # Prepare the LLM input as a list of dictionaries\n",
    "    llm_input = [{'id': feedback_id, 'feedback': feedback} for feedback_id, feedback in id_feedback.items()]\n",
    "\n",
    "    return llm_input, id_feedback\n",
    "\n",
    "\n",
    "def get_id_labels(llm_response: str, pattern: str = r'\\[\\s*\\{(?:.|\\n)*\\}\\s*\\]') -> List[Dict[str, str]]:\n",
    "    if not isinstance(llm_response, str):\n",
    "        raise TypeError(\"The LLM response must be a string.\")\n",
    "\n",
    "    try:\n",
    "        # Find the match\n",
    "        match = re.search(pattern, llm_response, re.DOTALL)\n",
    "        if not match:\n",
    "            print(f\"THIS RESPONSE WAS PRODUCED AND WAS UNABLE TO BE PICKED UP:\\n{llm_response}\")\n",
    "            raise ValueError(\"No valid JSON list found in the response.\")\n",
    "\n",
    "        json_string = match.group(0)\n",
    "        result = json.loads(json_string)\n",
    "\n",
    "        # Validate the structure of the result\n",
    "        if not isinstance(result, list) or not all(isinstance(item, dict) for item in result):\n",
    "            raise ValueError(\"Extracted JSON is not a list of dictionaries.\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Failed to decode JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def generate_batch_labels(id_feedback_pairs, label_prompt: str, client):\n",
    "    prompt = PromptTemplate(\n",
    "        template=label_prompt,\n",
    "        input_variables=[\"pairs\"],\n",
    "    )\n",
    "\n",
    "    final_prompt = prompt.format(pairs=id_feedback_pairs)\n",
    "\n",
    "    # Generate the completion by interacting with the language model API\n",
    "    completion = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": final_prompt\n",
    "                    }\n",
    "                    ],\n",
    "        temperature=0,  # Control the randomness of the output (lower means less random)\n",
    "        max_tokens=1024,  # Limit the response length\n",
    "        top_p=1,  # Nucleus sampling parameter (1 means only the most likely tokens are considered)\n",
    "        stream=True,  # Enable streaming of the response chunks\n",
    "        stop=None,  # Define stopping conditions (None means no stopping condition)\n",
    "    )\n",
    "\n",
    "    # Initialize an empty string to accumulate the response content\n",
    "    response = \"\"\"\"\"\"\n",
    "    for chunk in completion:\n",
    "        # Append each chunk of content to the response string\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        \n",
    "    pairings = get_id_labels(response)\n",
    "    \n",
    "    return pairings\n",
    "\n",
    "\n",
    "def generate_labels(llm_input, num_per_batch, output_file=f'../data/llm_responses/llm_responses.json'):\n",
    "    # Determine the number of batches\n",
    "    num_batches = ceil(len(llm_input) / num_per_batch)\n",
    "\n",
    "    # Initialise indices for batch processing\n",
    "    start_index = 0\n",
    "\n",
    "    just_in_case_stop_index = 0\n",
    "    \n",
    "    try:\n",
    "        for i in trange(num_batches):\n",
    "            # Calculate the batch indices\n",
    "            end_index = start_index + num_per_batch\n",
    "            batch_pairs = llm_input[start_index:end_index]\n",
    "            try:\n",
    "                \n",
    "                # Call the function to generate labels for the current batch\n",
    "                batch_labels = generate_batch_labels(batch_pairs, GENERATE_EN_LABELS_PROMPT, client)\n",
    "            except ValueError:\n",
    "                    intermediate_end = start_index+5\n",
    "                    batch_pairs = llm_input[start_index:intermediate_end]\n",
    "                    batch_labels = generate_batch_labels(batch_pairs, GENERATE_EN_LABELS_PROMPT, client)   \n",
    "                    \n",
    "                    # Write the current batch to the JSON file\n",
    "                    with open(output_file, 'a') as json_file:\n",
    "                        # Convert the batch to a JSON string and write it\n",
    "                        for label in batch_labels:\n",
    "                            json_file.write(json.dumps(label) + '\\n')\n",
    "                    \n",
    "                    intermediate_start = intermediate_end\n",
    "                    batch_pairs = llm_input[intermediate_start:end_index]\n",
    "                    batch_labels = generate_batch_labels(batch_pairs, GENERATE_EN_LABELS_PROMPT, client)   \n",
    "                    \n",
    "                    # Write the current batch to the JSON file\n",
    "                    with open(output_file, 'a') as json_file:\n",
    "                        # Convert the batch to a JSON string and write it\n",
    "                        for label in batch_labels:\n",
    "                            json_file.write(json.dumps(label) + '\\n')\n",
    "                    \n",
    "                    # Sleep for 60 seconds every 10 iterations\n",
    "                    if (i + 1) % 5 == 0:\n",
    "                        print(f\"Completed {i + 1} iterations. To prevent rate limits, sleeping for 60 seconds...\")\n",
    "                        time.sleep(60)\n",
    "                        \n",
    "                    continue\n",
    "                \n",
    "            # Update the start index for the next batch\n",
    "            start_index = end_index\n",
    "\n",
    "            # Write the current batch to the JSON file\n",
    "            with open(output_file, 'a') as json_file:\n",
    "                # Convert the batch to a JSON string and write it\n",
    "                for label in batch_labels:\n",
    "                    json_file.write(json.dumps(label) + '\\n')\n",
    "\n",
    "            # Sleep for 60 seconds every 10 iterations\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"Completed {i + 1} iterations. To prevent rate limits, sleeping for 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "                \n",
    "            just_in_case_stop_index = end_index\n",
    "            # Include  extra rest (not sure why but just in case lol)\n",
    "            time.sleep(2)\n",
    "\n",
    "        print(f\"All batches written to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"An error occurred while processing: {e}\")\n",
    "        print(f\"Stopped at batch {just_in_case_stop_index}\\n\")\n",
    "\n",
    "\n",
    "def read_json_file(file_path: str):\n",
    "    try:\n",
    "        data = []\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            for line in json_file:\n",
    "                data.append(json.loads(line.strip()))\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} does not exist.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "        \n",
    "def pair_id_feedback(id_feedback: dict, feedback_labels: list):\n",
    "    for i in range(len(feedback_labels)):\n",
    "        id = feedback_labels[i]['feedback_id']\n",
    "        comment = id_feedback[id]\n",
    "        feedback_labels[i]['Comment'] = comment\n",
    "        \n",
    "    return feedback_labels\n",
    "\n",
    "\n",
    "def write_to_csv(region: str, combined):\n",
    "    # Convert to a DataFrame\n",
    "    combined_df = pd.DataFrame(combined)\n",
    "\n",
    "    # Rename columns to match the required format\n",
    "    combined_df.rename(columns={'feedback_id': 'Feedback id',\n",
    "                                'label': 'Label',\n",
    "                                'Comment': 'Comment'},\n",
    "                        inplace=True)\n",
    "    combined_df['Comment'] = combined_df['Comment'].str.replace('\"\"', '\"', regex=False).str.strip('\"')\n",
    "\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_filename = f'../data/{region}_labelled_feedback_data.csv'\n",
    "    combined_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"{region} Labels wrote to csv\")\n",
    "\n",
    "def main():\n",
    "    region = \"SG\"\n",
    "    df = load_region_data(region)\n",
    "    llm_input, id_feedback = format_llm_input(df)\n",
    "    generate_labels(llm_input, num_per_batch=10)\n",
    "    feedback_labels = read_json_file(file_path=f'../data/llm_responses/llm_responses.json')\n",
    "    combined = pair_id_feedback(id_feedback, feedback_labels)\n",
    "    write_to_csv(region, combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pw/bwnktr8x7hv8c4fcp7js_f8r0000gr/T/ipykernel_27084/1251861346.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Feedback 2'] = df_filtered['Feedback 2'].apply(\n",
      "/var/folders/pw/bwnktr8x7hv8c4fcp7js_f8r0000gr/T/ipykernel_27084/1251861346.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Feedback id'] = pd.to_numeric(df_filtered['Feedback id'], errors='coerce')\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]2025-01-28 01:01:34,550 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  3%|▎         | 1/35 [00:01<00:40,  1.20s/it]2025-01-28 01:01:35,788 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  6%|▌         | 2/35 [00:02<00:40,  1.22s/it]2025-01-28 01:01:36,969 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  9%|▊         | 3/35 [00:03<00:37,  1.18s/it]2025-01-28 01:01:38,101 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 11%|█▏        | 4/35 [00:04<00:39,  1.26s/it]2025-01-28 01:01:39,487 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 5 iterations. To prevent rate limits, sleeping for 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5/35 [01:06<11:25, 22.86s/it]2025-01-28 01:02:40,704 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 17%|█▋        | 6/35 [01:07<07:30, 15.54s/it]2025-01-28 01:02:41,952 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 20%|██        | 7/35 [01:08<05:02, 10.80s/it]2025-01-28 01:02:43,018 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 23%|██▎       | 8/35 [01:09<03:27,  7.69s/it]2025-01-28 01:02:44,088 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 26%|██▌       | 9/35 [01:10<02:26,  5.64s/it]2025-01-28 01:02:45,209 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 10 iterations. To prevent rate limits, sleeping for 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 10/35 [02:11<09:28, 22.75s/it]2025-01-28 01:03:57,201 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 31%|███▏      | 11/35 [02:47<10:39, 26.67s/it]2025-01-28 01:04:21,878 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 34%|███▍      | 12/35 [02:48<07:14, 18.88s/it]2025-01-28 01:04:22,840 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 37%|███▋      | 13/35 [02:49<04:59, 13.59s/it]2025-01-28 01:04:24,332 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 40%|████      | 14/35 [02:50<03:26,  9.83s/it]2025-01-28 01:04:25,461 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 15 iterations. To prevent rate limits, sleeping for 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 15/35 [03:51<08:25, 25.28s/it]2025-01-28 01:05:26,500 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 46%|████▌     | 16/35 [03:53<05:42, 18.02s/it]2025-01-28 01:05:29,337 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 49%|████▊     | 17/35 [04:19<06:09, 20.54s/it]2025-01-28 01:06:15,250 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 51%|█████▏    | 18/35 [04:59<07:27, 26.34s/it]2025-01-28 01:06:33,885 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 54%|█████▍    | 19/35 [05:00<05:00, 18.80s/it]2025-01-28 01:06:35,105 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 20 iterations. To prevent rate limits, sleeping for 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 20/35 [06:01<07:52, 31.49s/it]2025-01-28 01:07:36,204 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 60%|██████    | 21/35 [06:02<05:13, 22.40s/it]2025-01-28 01:07:37,463 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 63%|██████▎   | 22/35 [06:03<03:27, 15.98s/it]2025-01-28 01:07:38,386 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 66%|██████▌   | 23/35 [06:04<02:17, 11.49s/it]2025-01-28 01:07:39,431 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 69%|██████▊   | 24/35 [06:06<01:33,  8.46s/it]2025-01-28 01:07:40,837 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 25 iterations. To prevent rate limits, sleeping for 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 25/35 [07:07<04:02, 24.28s/it]2025-01-28 01:08:42,560 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 74%|███████▍  | 26/35 [07:09<02:37, 17.52s/it]2025-01-28 01:08:43,932 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 77%|███████▋  | 27/35 [07:10<01:41, 12.64s/it]2025-01-28 01:08:45,003 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 80%|████████  | 28/35 [07:33<01:51, 15.90s/it]2025-01-28 01:09:08,500 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 83%|████████▎ | 29/35 [07:35<01:08, 11.44s/it]2025-01-28 01:09:09,553 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 30 iterations. To prevent rate limits, sleeping for 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 30/35 [08:36<02:11, 26.33s/it]2025-01-28 01:10:10,752 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 89%|████████▊ | 31/35 [08:37<01:15, 18.77s/it]2025-01-28 01:10:11,802 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 91%|█████████▏| 32/35 [08:38<00:40, 13.57s/it]2025-01-28 01:10:13,214 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 94%|█████████▍| 33/35 [08:39<00:19,  9.82s/it]2025-01-28 01:10:14,272 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 97%|█████████▋| 34/35 [08:40<00:07,  7.19s/it]2025-01-28 01:10:15,339 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 35 iterations. To prevent rate limits, sleeping for 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [09:41<00:00, 16.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All batches written to ../data/llm_responses/llm_responses.json\n",
      "SG Labels wrote to csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating rate limit per day (500,000 tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
