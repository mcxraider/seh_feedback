{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from groq import Groq\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from tqdm import trange\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import logging\n",
    "from math import ceil\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "## load evv variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "CHAT_MODEL   = os.environ[\"CHAT_MODEL\"]\n",
    "client       = Groq()\n",
    "\n",
    "GENERATE_EN_LABELS_PROMPT = '''\n",
    "You are an linguistics professor tasked with classifying seller feedback for an e-commerce platform. \n",
    "Each feedback item should be categorised into one or more appropriate labels from the following list:\n",
    "['Negative Complaint','Constructive Criticism','Design Feedback','Positive Comment','Neutral']\n",
    "You are not to write any code, but just use your knowledge to classify the feedback.\n",
    "Your output should be the feedback IDs and their corresponding label.\n",
    "\n",
    "Now classify the following feedback:\n",
    "Feedbacks: {pairs}\n",
    "\n",
    "Example Output format:\n",
    "[{{\"feedback_id\": 123456, \"label\": [\"Negative Complaint\"]}}, {{\"feedback_id\": 423456, \"label\": [\"Constructive Criticism\",\"Design Feedback\"]}}, {{\"feedback_id\": 654321, \"label\": [\"Negative Complaint\"]}}]\n",
    "\n",
    "Double check and ensure that your format output matches the example output format provided.\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_region_data(region: str) -> pd.DataFrame:\n",
    "    # Define the file path based on the region\n",
    "    region_path = f\"../data/official_data/feedback_{region}.xls\"\n",
    "\n",
    "    # Specify columns to read\n",
    "    columns_to_read = [\"Feedback id\", \"Feedback 1\", \"Feedback 2\"]\n",
    "\n",
    "    # Load the data into a DataFrame\n",
    "    df = pd.read_excel(region_path, usecols=columns_to_read)\n",
    "\n",
    "    # Filter out rows with missing or invalid data\n",
    "    df_filtered = df[\n",
    "        (df['Feedback 1'].notna()) &\n",
    "        (df['Feedback 2'].notna()) &\n",
    "        (df['Feedback 2'] != '{\"description\":\"\"}')\n",
    "    ]\n",
    "\n",
    "    # Extract the 'description' field from JSON in 'Feedback 2'\n",
    "    df_filtered['Feedback 2'] = df_filtered['Feedback 2'].apply(\n",
    "        lambda x: json.loads(x)['description'] if isinstance(x, str) else None\n",
    "    )\n",
    "\n",
    "    # Convert 'Feedback id' to numeric and drop rows with invalid IDs\n",
    "    df_filtered['Feedback id'] = pd.to_numeric(df_filtered['Feedback id'], errors='coerce')\n",
    "    df_filtered = df_filtered.dropna(subset=['Feedback id'])\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def format_llm_input(df: pd.DataFrame) -> Tuple[List[Dict[str, str]], Dict[int, str]]:\n",
    "    # Extract feedback IDs and feedback text\n",
    "    feedback_ids = list(df['Feedback id'])\n",
    "    feedback_texts = list(df['Feedback 2'])\n",
    "\n",
    "    # Create a dictionary mapping feedback IDs to feedback text\n",
    "    id_feedback = {int(feedback_id): feedback for feedback_id, feedback in zip(feedback_ids, feedback_texts)}\n",
    "\n",
    "    # Prepare the LLM input as a list of dictionaries\n",
    "    llm_input = [{'id': feedback_id, 'feedback': feedback} for feedback_id, feedback in id_feedback.items()]\n",
    "\n",
    "    return llm_input, id_feedback\n",
    "\n",
    "\n",
    "def get_id_labels(llm_response: str, pattern: str = r'\\[\\s*\\{(?:.|\\n)*\\}\\s*\\]') -> List[Dict[str, str]]:\n",
    "    if not isinstance(llm_response, str):\n",
    "        raise TypeError(\"The LLM response must be a string.\")\n",
    "\n",
    "    try:\n",
    "        # Find the match\n",
    "        match = re.search(pattern, llm_response, re.DOTALL)\n",
    "        if not match:\n",
    "            print(f\"THIS RESPONSE WAS PRODUCED AND WAS UNABLE TO BE PICKED UP:\\n{llm_response}\")\n",
    "            raise ValueError(\"No valid JSON list found in the response.\")\n",
    "\n",
    "        json_string = match.group(0)\n",
    "        result = json.loads(json_string)\n",
    "\n",
    "        # Validate the structure of the result\n",
    "        if not isinstance(result, list) or not all(isinstance(item, dict) for item in result):\n",
    "            raise ValueError(\"Extracted JSON is not a list of dictionaries.\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Failed to decode JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def generate_batch_labels(id_feedback_pairs, label_prompt: str, client):\n",
    "    prompt = PromptTemplate(\n",
    "        template=label_prompt,\n",
    "        input_variables=[\"pairs\"],\n",
    "    )\n",
    "\n",
    "    final_prompt = prompt.format(pairs=id_feedback_pairs)\n",
    "\n",
    "    # Generate the completion by interacting with the language model API\n",
    "    completion = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": final_prompt\n",
    "                    }\n",
    "                    ],\n",
    "        temperature=0,  # Control the randomness of the output (lower means less random)\n",
    "        max_tokens=1024,  # Limit the response length\n",
    "        top_p=1  # Nucleus sampling parameter (1 means only the most likely tokens are considered)\n",
    "    )\n",
    "\n",
    "    # Initialize an empty string to accumulate the response content\n",
    "    response = completion.choices[0].message.content\n",
    "\n",
    "    tokens_used = completion.usage.total_tokens\n",
    "    pairings = get_id_labels(response)\n",
    "    \n",
    "    return pairings, tokens_used\n",
    "\n",
    "\n",
    "def generate_labels(llm_input, num_per_batch, output_file=f'../data/llm_responses/llm_responses.json'):\n",
    "    # Determine the number of batches\n",
    "    num_batches = ceil(len(llm_input) / num_per_batch)\n",
    "\n",
    "    # Initialise indices for batch processing\n",
    "    start_index = 0\n",
    "\n",
    "    just_in_case_stop_index = 0\n",
    "    \n",
    "    total_tokens = 0\n",
    "    \n",
    "    try:\n",
    "        for i in trange(num_batches):\n",
    "            # Calculate the batch indices\n",
    "            end_index = start_index + num_per_batch\n",
    "            batch_pairs = llm_input[start_index:end_index]\n",
    "            try:\n",
    "                \n",
    "                # Call the function to generate labels for the current batch\n",
    "                batch_labels, tokens_used = generate_batch_labels(batch_pairs, GENERATE_EN_LABELS_PROMPT, client)\n",
    "                total_tokens += tokens_used\n",
    "            except ValueError:\n",
    "                    intermediate_end = start_index+5\n",
    "                    batch_pairs = llm_input[start_index:intermediate_end]\n",
    "                    batch_labels, tokens_used = generate_batch_labels(batch_pairs, GENERATE_EN_LABELS_PROMPT, client)   \n",
    "                    total_tokens += tokens_used\n",
    "\n",
    "                    # Write the current batch to the JSON file\n",
    "                    with open(output_file, 'a') as json_file:\n",
    "                        # Convert the batch to a JSON string and write it\n",
    "                        for label in batch_labels:\n",
    "                            json_file.write(json.dumps(label) + '\\n')\n",
    "                    \n",
    "                    intermediate_start = intermediate_end\n",
    "                    batch_pairs = llm_input[intermediate_start:end_index]\n",
    "                    batch_labels, tokens_used = generate_batch_labels(batch_pairs, GENERATE_EN_LABELS_PROMPT, client)   \n",
    "                    total_tokens += tokens_used\n",
    "\n",
    "                    # Write the current batch to the JSON file\n",
    "                    with open(output_file, 'a') as json_file:\n",
    "                        # Convert the batch to a JSON string and write it\n",
    "                        for label in batch_labels:\n",
    "                            json_file.write(json.dumps(label) + '\\n')\n",
    "                    \n",
    "                    # Sleep for 60 seconds every 10 iterations\n",
    "                    if (i + 1) % 5 == 0:\n",
    "                        print(f\"Completed {i + 1} iterations. To prevent rate limits, sleeping for 60 seconds...\")\n",
    "                        time.sleep(60)\n",
    "                        \n",
    "                    continue\n",
    "                \n",
    "            # Update the start index for the next batch\n",
    "            start_index = end_index\n",
    "\n",
    "            # Write the current batch to the JSON file\n",
    "            with open(output_file, 'a') as json_file:\n",
    "                # Convert the batch to a JSON string and write it\n",
    "                for label in batch_labels:\n",
    "                    json_file.write(json.dumps(label) + '\\n')\n",
    "\n",
    "            # Sleep for 60 seconds every 10 iterations\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"Completed {i + 1} iterations. To prevent rate limits, sleeping for 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "                \n",
    "            just_in_case_stop_index = end_index\n",
    "            # Include  extra rest (not sure why but just in case lol)\n",
    "            time.sleep(2)\n",
    "\n",
    "        print(f\"All batches written to {output_file}\")\n",
    "        return total_tokens\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"An error occurred while processing: {e}\")\n",
    "        print(f\"Stopped at batch {just_in_case_stop_index}\\n\")\n",
    "\n",
    "\n",
    "def read_json_file(file_path: str):\n",
    "    try:\n",
    "        data = []\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            for line in json_file:\n",
    "                data.append(json.loads(line.strip()))\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} does not exist.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "        \n",
    "def pair_id_feedback(id_feedback: dict, feedback_labels: list):\n",
    "    for i in range(len(feedback_labels)):\n",
    "        id = feedback_labels[i]['feedback_id']\n",
    "        comment = id_feedback[id]\n",
    "        feedback_labels[i]['Comment'] = comment\n",
    "        \n",
    "    return feedback_labels\n",
    "\n",
    "\n",
    "def write_to_csv(region: str, combined):\n",
    "    # Convert to a DataFrame\n",
    "    combined_df = pd.DataFrame(combined)\n",
    "\n",
    "    # Rename columns to match the required format\n",
    "    combined_df.rename(columns={'feedback_id': 'Feedback id',\n",
    "                                'label': 'Label',\n",
    "                                'Comment': 'Comment'},\n",
    "                        inplace=True)\n",
    "    combined_df['Comment'] = combined_df['Comment'].str.replace('\"\"', '\"', regex=False).str.strip('\"')\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_filename = f'../data/labelled_feedback/{region}_labelled_feedback_data.csv'\n",
    "    combined_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"{region} Labels are now store in the data folder under labelled_feedback.\")\n",
    "\n",
    "def main():\n",
    "    region = \"SG\"\n",
    "    df = load_region_data(region)\n",
    "    llm_input, id_feedback = format_llm_input(df)\n",
    "    # Plan on what to do with this token consumed.\n",
    "    total_tokens_consumed = generate_labels(llm_input, num_per_batch=10)\n",
    "    feedback_labels = read_json_file(file_path=f'../data/llm_responses/{region}_llm_responses.json')\n",
    "    combined = pair_id_feedback(id_feedback, feedback_labels)\n",
    "    write_to_csv(region, combined)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pw/bwnktr8x7hv8c4fcp7js_f8r0000gr/T/ipykernel_27084/3519781230.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Feedback 2'] = df_filtered['Feedback 2'].apply(\n",
      "/var/folders/pw/bwnktr8x7hv8c4fcp7js_f8r0000gr/T/ipykernel_27084/3519781230.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Feedback id'] = pd.to_numeric(df_filtered['Feedback id'], errors='coerce')\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]2025-01-28 01:22:06,552 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  3%|▎         | 1/35 [00:03<01:50,  3.26s/it]2025-01-28 01:22:10,930 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  3%|▎         | 1/35 [00:07<04:14,  7.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 229\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m llm_input, id_feedback \u001b[38;5;241m=\u001b[39m format_llm_input(df)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Plan on what to do with this token consumed.\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m total_tokens_consumed \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_per_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m feedback_labels \u001b[38;5;241m=\u001b[39m read_json_file(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/llm_responses/llm_responses.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    231\u001b[0m combined \u001b[38;5;241m=\u001b[39m pair_id_feedback(id_feedback, feedback_labels)\n",
      "Cell \u001b[0;32mIn[6], line 172\u001b[0m, in \u001b[0;36mgenerate_labels\u001b[0;34m(llm_input, num_per_batch, output_file)\u001b[0m\n\u001b[1;32m    170\u001b[0m     just_in_case_stop_index \u001b[38;5;241m=\u001b[39m end_index\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# Include  extra rest (not sure why but just in case lol)\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll batches written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_tokens\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating rate limit per day (500,000 tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
